{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13a2aa8c-37bc-43a0-b61f-46049ac25ff6",
   "metadata": {},
   "source": [
    "# **Key Concepts in Neural Networks and Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f1ee60-6628-4b6f-8ef1-f2d799fce9d7",
   "metadata": {},
   "source": [
    "## 1. Biases\n",
    "\n",
    "In the context of neural networks, biases are additional parameters associated with each neuron. **They act as an offset, allowing the neuron to adjust its output regardless of the input values.** Biases provide flexibility in the decision-making process of the network.\n",
    "\n",
    "Mathematically, biases are represented as individual terms in the neuron's calculations. They are added to the weighted sum before applying the activation function.\n",
    "\n",
    "Example: In the calculation of the weighted sum z = w1x1 + w2x2 + w3*x3, biases can be introduced as additional terms b1, b2, b3. The neuron's output is then obtained by applying the activation function to the sum with biases: **output = sigmoid(z + b)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a06558e-588b-4baf-83c6-68fc8cae05f1",
   "metadata": {},
   "source": [
    "## 2. Loss function\n",
    "Also known as a cost function or objective function, is a measure of how well a machine learning model is performing on a given task. It quantifies the discrepancy between the predicted output of the model and the actual target output.  It serves as a guide for the model to optimize its parameters during training.\n",
    "\n",
    "Three common loss functions are:\n",
    "\n",
    "**- Mean Squared Error (MSE):**\n",
    "\n",
    "MSE calculates the average squared difference between the predicted output (ŷ) and the true target output (y). It is commonly used in regression tasks. Math notation: **MSE = (1/n) * Σ(y - ŷ)^2**\n",
    "\n",
    "**- Binary Cross-Entropy:**\n",
    "\n",
    "Binary cross-entropy is used for binary classification problems. It measures the dissimilarity between the predicted probabilities (ŷ) and the true binary labels (y). Math notation: **BCE = -[y * log(ŷ) + (1-y) * log(1-ŷ)]**\n",
    "\n",
    "**- Categorical Cross-Entropy:**\n",
    "\n",
    "Categorical cross-entropy is used for multi-class classification problems. It quantifies the difference between the predicted class probabilities (ŷ) and the true class labels (y). Math notation: **CCE = -Σ(y * log(ŷ))**\n",
    "\n",
    "\n",
    "\n",
    "In the mathematical notations, ŷ represents the predicted output, y represents the true target output, and n represents the number of samples in the dataset. The model aims to minimize the value of the loss function during training to improve its predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85add4df-8683-44cf-a83f-3e86e1bfc112",
   "metadata": {},
   "source": [
    "## **3. Feed-Forward**\n",
    "Feed-forward is the process of **passing input data through the neural network, from the input layer to the output layer.** Each neuron in the network receives input, performs calculations using weights, and produces an output that serves as input to the next layer. This process continues until the final prediction is made.\n",
    "\n",
    "Example: In our fruit classification network, during feed-forward, we input the color and shape of a fruit. Each neuron in the network processes this information and passes it to the next layer until we get the final prediction of whether it's an apple or an orange."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1069f4e-c870-41cf-8363-3ef4df72dee0",
   "metadata": {},
   "source": [
    "4. Backpropagation\n",
    "Backpropagation is an algorithm used to **train the neural network by adjusting its parameters, such as weights and biases. It helps the network learn from its mistakes and improve its predictions.\n",
    "\n",
    "Example: During training, we compare the network's predicted class with the true class for many fruits. Backpropagation calculates how much each parameter (weight and bias) contributed to the errors. The network adjusts these parameters to reduce the errors and improve its predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4799f10-ec39-4026-b042-0e96aa5b897f",
   "metadata": {},
   "source": [
    "5. Gradient Descent\n",
    "Gradient descent is an optimization algorithm used to minimize a loss function during the training of neural networks. It works by iteratively adjusting the network's parameters, such as weights and biases, based on the gradients of the loss function.\n",
    "\n",
    "The gradients represent the slope or direction of steepest descent in the loss landscape. They provide information about the changes needed in the parameters to reduce the loss. By knowing the direction and steepness of the slope at a particular parameter value, we can make adjustments that lead to a decrease in the loss.\n",
    "\n",
    "To understand this concept, imagine the loss function as a hilly landscape, where the goal is to reach the lowest point representing the minimum loss. The gradients act as a compass, guiding us towards the steepest downhill direction. Just as you would naturally choose the steepest path when descending a hill, gradient descent helps us move in the direction that leads to the most significant reduction in the loss function.\n",
    "\n",
    "By calculating the gradients, we obtain valuable information about the changes required in the parameters. The magnitude of the gradient indicates the step size or the amount by which we adjust the parameters. Large gradients suggest significant changes, while smaller gradients imply more subtle adjustments.\n",
    "\n",
    "During the iterative process of gradient descent, we follow the gradients and update the parameters accordingly, moving closer to the minimum of the loss function. This process continues until convergence is reached, where the loss is minimized or no significant improvement is observed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c18659-60db-48e9-9a40-a53e68f3e80f",
   "metadata": {},
   "source": [
    "6. Epoch\n",
    "An epoch refers to a complete pass or iteration through the entire training dataset during the training phase. In other words, one epoch means that the model has seen and processed every example in the training set once. \n",
    "\n",
    "During an epoch, the model's parameters, such as weights and biases, are updated based on the gradients computed during the forward and backward propagation steps of the training algorithm. The goal of training is to iteratively improve the model's performance by adjusting these parameters to minimize the loss function, which measures the discrepancy between the model's predictions and the actual target values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354ef359-bb1a-4ea7-ac63-2f606aad1a00",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
