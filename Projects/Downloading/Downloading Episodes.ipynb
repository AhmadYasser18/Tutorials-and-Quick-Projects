{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d50c0af2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-15T17:44:46.271290Z",
     "start_time": "2022-08-15T17:44:46.263294Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "45a576e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-15T19:36:34.863631Z",
     "start_time": "2022-08-15T19:36:34.847617Z"
    }
   },
   "outputs": [],
   "source": [
    "link='https://gogoanime.lu/one-piece-episode-1016'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "275452e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-15T19:14:00.844201Z",
     "start_time": "2022-08-15T19:13:59.756299Z"
    }
   },
   "outputs": [],
   "source": [
    "html_file=requests.get(link).text\n",
    "soup = BeautifulSoup(html_file, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b8645ef1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-15T19:14:54.553677Z",
     "start_time": "2022-08-15T19:14:54.529640Z"
    }
   },
   "outputs": [],
   "source": [
    "#Enter starting chapter\n",
    "ep=str(1016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "545f72b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-15T19:18:29.888399Z",
     "start_time": "2022-08-15T19:18:29.864394Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('div',_class='anime_muti_link')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f714b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in chapters.find_all('a'):\n",
    "    if ch==c.text.split()[-1]:\n",
    "        ch_link=c['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b6cd7c53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-15T19:49:38.051422Z",
     "start_time": "2022-08-15T19:49:38.046425Z"
    }
   },
   "outputs": [],
   "source": [
    "anime='One Piece'\n",
    "episode=1021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "db55a29d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-15T19:45:39.546890Z",
     "start_time": "2022-08-15T19:45:39.530891Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(f'{anime}')\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "16c83ed1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-15T20:01:14.059721Z",
     "start_time": "2022-08-15T19:54:24.120237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1017 done\n",
      "1018 done\n",
      "1019 done\n",
      "1020 done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "url=['https://fvs.io/redirector?token=SHE4NFFtTko1MExycUpacU1kVTNDZzJNZTlhc3BjNWd4eWxnTkw5ajFtaExhMFlkY1hBMU1hSlV5dDRTaE5YbHg1SnJZUkMwVzBqM3hNY0dOejAzSUZLTjFaZVdOOXp6SXM2WWZZSldTam9GK0ZjVGNFY0ViMUVBNURxSHY1SHp5bUVrZU5DdU1ZaER6KzZMNmpaaU0zT1dmY2pDS1FIK3lnPT06a2cwVnNtbmlpT2JiSjNIM0Y3R0V2dz09HjXm',\n",
    "    'https://fvs.io/redirector?token=eEg5aStCcDRzc1pJUmJkWHVIckUrRGZyVnpvWFpwVVczSkFyTTFUbUdQRHA1elBoU3FXNUpyTlJob0d1MDUyRE43RkpYeHVuSmtGWEw0eWUzTFZkb1g0Qk55dnJmdCs2VWpDdDNiWjZMZzFWRXNlSkwzNmM5RzRaLzdqRGc2c1pENXlXR3cwMGJ1OUZBRU5kL2JlTXhMUEF5UzRHS1c5TEVBPT06T0tyYTFxUC9PbjgwTUlUblRpZjF1dz09jxZh',\n",
    "    'https://fvs.io/redirector?token=YUdnZEdBSHhUZ0dvd1dUR0YxT1A1RjVET1VKSURycEdEbURZN0VrU0k2V1FUQUQzdDJoYko3MGorbjg4ajViVXIrbGlDRmNBQ2syUld6U1lLY0FWY29lQzRYd2VZMXh2YlQwQ2hxbjIyNGhqbDF0bVg2b0ZXQTM3UExXdVdQSHVDQWkyNkZSTnJFNWtXbkRXS1RRMWtDSTAyYkt5dE9nNzZBPT06RTNYU3BPZjNVNmx4QjVFT2QwK01LZz09GE6V',\n",
    "    'https://fvs.io/redirector?token=VzdrQUxaelZHUm1nQTVDektaK0JtSjYrNm5LZk1uL3NTM0dnNThBbG1EY1IvaEI2TnNmRWFnL2lUbWMyWnd0YlEzVU1ZbDlxcDhiaUtScFVOWFgzYjFNcmRlbFQycGhaMENEUkVRdmV3T1V4eXROVGJETWRDOG5SUHpuSFlZYjZEajl0Um9OQlU2NWF0cThhZW43eUNzZEh4N1hnSVYvYzpIZERXcUdZNmNXNjcwbmM3SkZiQ2tRPT01GDM']\n",
    "for u in url:\n",
    "    chunk=256\n",
    "    r=requests.get(u,stream=True)\n",
    "\n",
    "    with open(f'{anime}/{anime} episode {episode}.mp4','wb') as f:\n",
    "        for c in r.iter_content(chunk_size=chunk):\n",
    "            f.write(c)\n",
    "    print(f'{episode} done')\n",
    "    episode+=1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3656e6c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-01T21:51:57.383332Z",
     "start_time": "2022-08-01T21:50:59.348514Z"
    }
   },
   "outputs": [],
   "source": [
    "#Enter number of wanted chapters\n",
    "chaps_num= \n",
    "\n",
    "counter=0\n",
    "\n",
    "while True:\n",
    "    \n",
    "    html_file=requests.get(ch_link).text\n",
    "    soup = BeautifulSoup(html_file, 'lxml')\n",
    "    \n",
    "    chapter=' '.join(soup.title.text.split()[2:4])\n",
    "    print(chapter)\n",
    "    \n",
    "    panels=[i['src'] for i in soup.find_all('img') if 'chapter' in i['alt']]\n",
    "    p=1\n",
    "    part=1\n",
    "    print(f'passed panels {len(panels)}')\n",
    "    panelslist=[]\n",
    "    first=True\n",
    "    for i,j in enumerate(panels):\n",
    "        print(f'in panel:{p}')\n",
    "        try:\n",
    "            image_content=requests.get(j).content\n",
    "        except:\n",
    "            print(f'Error, panel:{p}//{len(panels)}')\n",
    "            image1.save(f'{chapter}part{part}.pdf', \"PDF\" ,resolution=100.0, save_all=True, append_images=panelslist)\n",
    "            part+=1\n",
    "            first=True\n",
    "            continue\n",
    "            break\n",
    "        image_file=io.BytesIO(image_content)\n",
    "            \n",
    "        if first:\n",
    "            image1=Image.open(image_file)\n",
    "            p+=1\n",
    "            first=False\n",
    "            print(f'finished panel:{p}')\n",
    "            continue\n",
    "                \n",
    "        panelslist.append(Image.open(image_file))\n",
    "        print(f'finished panel:{p}')\n",
    "        p+=1\n",
    "        \n",
    "        \n",
    "    image1.save(f'{chapter}.pdf', \"PDF\" ,resolution=100.0, save_all=True, append_images=panelslist)        \n",
    "            \n",
    "    \n",
    "    counter+=1\n",
    "        \n",
    "    if counter==chaps_num:\n",
    "        break\n",
    "    \n",
    "    try:\n",
    "        ch_link=soup.find('div' ,class_=\"col-md-6 next-post\").find('a')['href']\n",
    "    \n",
    "    except:      \n",
    "        print(f'Done.\\n{counter} downloaded')\n",
    "        break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72814db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "\n",
    "while True:\n",
    "    \n",
    "    html_file=requests.get(ch_link).text\n",
    "    soup = BeautifulSoup(html_file, 'lxml')\n",
    "    \n",
    "    chapter=' '.join(soup.title.text.split()[2:4])\n",
    "    print(chapter)\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(f'{chapter}')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    panels=[i['src'] for i in soup.find_all('img') if 'reader' in i['src']]\n",
    "    p=0\n",
    "    print('passed panels')\n",
    "    for i,j in enumerate(panels):\n",
    "        try:\n",
    "            image_content=requests.get(j).content\n",
    "        except:\n",
    "            print('Error')\n",
    "            break\n",
    "        image_file=io.BytesIO(image_content)\n",
    "            \n",
    "        if p==0:\n",
    "            image1=Image.open(image_file)\n",
    "            p+=1\n",
    "            continue\n",
    "                \n",
    "        image2=Image.open(image_file)\n",
    "\n",
    "        \n",
    "        if image1.size[0]!=image2.size[0]: \n",
    "            continue\n",
    "            ratio= image1.size[0]//image2.size[0]\n",
    "            image1 = image1.resize((image1.size[0]//ratio, image1.size[1]//ratio))\n",
    "            \n",
    "        new_image=Image.new('RGB',(image1.size[0], image1.size[1]+image2.size[1]), (250,250,250))\n",
    "        new_image.paste(image1,(0,0))\n",
    "        new_image.paste(image2,(0,image1.size[1]))\n",
    "        try:\n",
    "            new_image.save(f'{i}.jpg')\n",
    "            image1=new_image\n",
    "            print(f'panel{p}')\n",
    "        except:\n",
    "            image2.save('problem.jpg')\n",
    "            image1=image2 \n",
    "            print(f'panel{p+1}')\n",
    "            if input('Enter')=='1':\n",
    "                break\n",
    "        finally:\n",
    "            print(p+1)\n",
    "                \n",
    "\n",
    "            \n",
    "    with open(f'{chapter}//1.jpg','wb') as f:\n",
    "        new_image.save(f,new_image.format)\n",
    "            \n",
    "    \n",
    "    counter+=1\n",
    "        \n",
    "    if counter==1:\n",
    "        break\n",
    "    \n",
    "    try:\n",
    "        ch_link=soup.find('div' ,class_=\"col-md-6 next-post\").find('a')['href']\n",
    "    \n",
    "    except:      \n",
    "        print(f'Done.\\n{counter} downloaded')\n",
    "        break\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
